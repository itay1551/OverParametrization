{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\itayk\\miniconda3\\envs\\Dcv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from vit_pytorch import ViT\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from model import MyViT\n",
    "from torch.utils.data import DataLoader, random_split, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for PyTorch\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from vit_pytorch import ViT\n",
    "\n",
    "# vit_model = MyViT(\n",
    "#     image_size = 32,\n",
    "#     patch_size = 8,\n",
    "#     num_classes = 1000,\n",
    "#     dim = 1500,\n",
    "#     depth = 6,\n",
    "#     heads = 16,\n",
    "#     mlp_dim = 1500 * 3,\n",
    "#     dropout = 0,\n",
    "#     # dropout = 0.1,\n",
    "#     emb_dropout = 0,\n",
    "#     # emb_dropout = 0.1,\n",
    "#     channels = 3\n",
    "# )\n",
    "\n",
    "# cifar_8_noise_no_final_layer_3\n",
    "vit_model = MyViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 8,\n",
    "    num_classes = 1000,\n",
    "    dim = 1028,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 1028 * 3,\n",
    "    dropout = 0,\n",
    "    # dropout = 0.1,\n",
    "    emb_dropout = 0,\n",
    "    # emb_dropout = 0.1,\n",
    "    channels = 3\n",
    ")\n",
    "\n",
    "img = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "# BIG\n",
    "# preds = v(img) # (1, 1000)\n",
    "# import torch\n",
    "# # from vit_pytorch import ViT\n",
    "\n",
    "# vit_model = MyViT(\n",
    "#     image_size = 28,\n",
    "#     patch_size = 7,\n",
    "#     num_classes = 1000,\n",
    "#     dim = 2056,\n",
    "#     depth = 4,\n",
    "#     heads = 6,\n",
    "#     mlp_dim = 2056 * 3,\n",
    "#     dropout = 0,\n",
    "#     # dropout = 0.1,\n",
    "#     emb_dropout = 0,\n",
    "#     # emb_dropout = 0.1,\n",
    "#     channels = 1\n",
    "# )\n",
    "\n",
    "# img = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "# # preds = v(img) # (1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 64812864\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "total_params = count_parameters(vit_model)\n",
    "print(f'Total number of parameters: {total_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./CIFAR_data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [01:32<00:00, 1846154.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./CIFAR_data\\cifar-10-python.tar.gz to ./CIFAR_data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "# # Download and load the test data\n",
    "# testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR_data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "train_size = 128\n",
    "train_dataset, test_dataset = random_split(trainset, [train_size, len(trainset) - train_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images = next(iter(train_loader))[0]\n",
    "test_samples  = next(iter(test_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_images(images):\n",
    "    \"\"\"\n",
    "    Prints a batch of images with shape (64, 1, 28, 28) or (64, 3, 28, 28).\n",
    "\n",
    "    Parameters:\n",
    "    images (torch.Tensor): A tensor of shape (64, C, H, W) where C can be 1 (grayscale) or 3 (RGB).\n",
    "    \"\"\"\n",
    "    # Ensure the input is a 4D tensor\n",
    "    if images.ndim == 5:\n",
    "        images = torch.squeeze(images, dim=1)\n",
    "    assert images.ndim == 4, \"Input tensor must be 4-dimensional\"\n",
    "    # Ensure the number of channels is either 1 or 3\n",
    "    assert images.shape[1] in [1, 3], \"Input tensor must have 1 or 3 channels\"\n",
    "    \n",
    "    # Get the number of images in the batch\n",
    "    batch_size = images.shape[0]\n",
    "    \n",
    "    # Define the number of rows and columns for the plot grid\n",
    "    n_cols = 8\n",
    "    n_rows = (batch_size + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Create a figure to hold the subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_rows * 1.5))\n",
    "    \n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Get the image and convert it to a numpy array\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Normalize the image to [0, 1] range\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        \n",
    "        # If the image has 1 channel, remove the last dimension\n",
    "        if img.shape[-1] == 1:\n",
    "            img = img.squeeze(-1)\n",
    "        \n",
    "        # Plot the image\n",
    "        axes[i].imshow(img, cmap='gray' if images.shape[1] == 1 else None)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Remove any remaining empty subplots\n",
    "    for i in range(batch_size, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = './cifar_8_noise_no_final_layer_4/epoch_12500_model_0.00000153.pth'\n",
    "checkpoint_path = './epoch_102500_model_0.00000156.pth'\n",
    "import os\n",
    "if not os.path.isfile(checkpoint_path):\n",
    "    raise FileNotFoundError(f\"Checkpoint file '{checkpoint_path}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vit_model.to(device)\n",
    "\n",
    "# Parameters for Gaussian noise\n",
    "mean = 0\n",
    "std_dev = 0.5  # Standard deviation (adjust as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available and use it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(vit_model.parameters(), lr=0.00000015)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "milestones = [150000]\n",
    "milestones = [500, 1000, 2000, 3000, 5000, 15000, 20000, 25000, 30000 , 40000 , 50000, 80000]\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.6)\n",
    "\n",
    "# Move the model to the GPU\n",
    "vit_model.to(device)\n",
    "\n",
    "num_epochs = 1_600_000\n",
    "\n",
    "# Parameters for Gaussian noise\n",
    "mean = 0\n",
    "std_dev = 0.5  \n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, optimizer, scheduler, dataloader, target_loss=1e-8):\n",
    "    model.train()\n",
    "    with tqdm(total=num_epochs, desc=\"Processing\") as pbar:\n",
    "        noise_counter = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for images, _ in dataloader:\n",
    "                # Move the images to the GPU\n",
    "                images = images.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                # Add Gaussian noise\n",
    "                noises = torch.normal(mean, std_dev, size=images.shape[-2:]).to(device)\n",
    "\n",
    "\n",
    "                noisy_images = images + noises\n",
    "                outputs = model(noisy_images)\n",
    "                loss = criterion(outputs, images)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            avg_loss = running_loss / len(dataloader)\n",
    "            pbar.set_postfix({'Loss:': f'{avg_loss}'})\n",
    "            pbar.update(1)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            if epoch % 2500 == 0 and epoch != 0:\n",
    "                torch.save(model.state_dict(), f'./cifar_8_noise_no_final_layer_4/epoch_{epoch}_model_{avg_loss:.8f}.pth')\n",
    "\n",
    "            # Save intermediate models\n",
    "            if avg_loss <= target_loss:\n",
    "                torch.save(model.state_dict(), f'./cifar_8_noise_no_final_layer_4/model_{avg_loss:.8f}.pth')\n",
    "                break\n",
    "\n",
    "# Train the model\n",
    "train_model(vit_model, optimizer, scheduler, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_herizontal_noise(img, number_of_lines=5, line_width=2, noise_value=None, return_noise=False):\n",
    "    h, w = img.shape[-2:]\n",
    "    white_space_size = 1\n",
    "    square_size = number_of_lines * (2 + white_space_size) # 1 is the space between lines\n",
    "    start_pos   = (h - square_size) // 2\n",
    "    end_pos = start_pos + square_size\n",
    "\n",
    "    noise = noise_value if noise_value else 2 * (torch.rand((line_width, square_size)) - 1)\n",
    "\n",
    "    img_noisy = img.clone()\n",
    "    noise_out = torch.ones((h, w)) * -1\n",
    "    for i in range(number_of_lines):\n",
    "        img_noisy[:,:,start_pos + (i * (line_width + white_space_size)): start_pos + (i * (line_width + white_space_size)) + line_width, start_pos: end_pos] = noise\n",
    "        noise_out[start_pos + (i * (line_width + white_space_size)): start_pos + (i * (line_width + white_space_size)) + line_width, start_pos: end_pos] = noise\n",
    "\n",
    "    if return_noise:\n",
    "        return img_noisy, noise_out\n",
    "    return img_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_side_noise(img, noise_value=None, return_noise=False):\n",
    "    h, w = img.shape[-2:]\n",
    "    noise = 2 * (torch.rand((h, w // 4)) - 1)\n",
    "\n",
    "\n",
    "    img_noisy = img.clone()\n",
    "    img_noisy[:,:,:, ((w + 1) // 4) * 3:] = noise_value if noise_value else noise\n",
    "    noise_out = torch.ones((h, w)) * -1\n",
    "    noise_out[:, ((w + 1) // 4) * 3:] = noise_value if noise_value else noise\n",
    "\n",
    "    if return_noise:\n",
    "        return img_noisy, noise_out\n",
    "    return img_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottom_noise(img, noise_value=None, return_noise=False):\n",
    "    h, w = img.shape[-2:]\n",
    "    noise = 2 * (torch.rand((h // 4, w)) - 1)\n",
    "\n",
    "\n",
    "    img_noisy = img.clone()\n",
    "    \n",
    "    img_noisy[:,:, ((h + 1) // 4) * 3:,:] = noise_value if noise_value else noise\n",
    "    noise_out = torch.ones((h, w)) * -1\n",
    "    noise_out[((h + 1) // 4) * 3:,:] = noise_value if noise_value else noise\n",
    "\n",
    "    if return_noise:\n",
    "        return img_noisy, noise_out\n",
    "    return img_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_noise(img, noise_size = 14, noise_value=None, return_noise=False):\n",
    "    h, w = img.shape[-2:]\n",
    "\n",
    "    img_noisy = img.clone()\n",
    "\n",
    "    start_pos = (h - noise_size) // 2\n",
    "    end_pos = start_pos + noise_size\n",
    "\n",
    "    noise = noise_value if noise_value else 2 * (torch.rand((noise_size, noise_size)) - 0.9)\n",
    "    img_noisy[:,:,start_pos: end_pos, start_pos:end_pos] = noise\n",
    "    m = torch.ones((h, w)) * -1\n",
    "    m[start_pos: end_pos, start_pos:end_pos] = noise\n",
    "    noise = m\n",
    "    if return_noise:\n",
    "        return img_noisy, noise\n",
    "    return img_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_noise(img, width=5, noise_value=1, return_noise=False):\n",
    "    h, w = img.shape[-2:]\n",
    "\n",
    "    img_noisy = img.clone()\n",
    "    noise = torch.ones((h, w)) * -1\n",
    "    for i in range(h-width):\n",
    "        noise[i:i+width,i] = noise_value\n",
    "        img_noisy[:,:,i:i+width,i] = noise_value\n",
    "        \n",
    "    if return_noise:\n",
    "        return img_noisy, noise\n",
    "    return img_noisy\n",
    "\n",
    "def tringle_noise (img, width=11, noise_value=1, return_noise=False):\n",
    "    h, w = img.shape[-2:]\n",
    "\n",
    "    start_row = w // 2\n",
    "    start_col = h // 4\n",
    "\n",
    "    noise = torch.ones((h, w)) * -1\n",
    "    img_noisy = img.clone()\n",
    "    for i in range(width):\n",
    "        noise[start_col+i,start_row-i:start_row+i] = noise_value\n",
    "        img_noisy[:,:,start_col+i,start_row-i:start_row+i] = noise_value\n",
    "    \n",
    "    if return_noise:\n",
    "        return img_noisy, noise\n",
    "    return img_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_noise(images, noise_func):\n",
    "    with torch.no_grad():\n",
    "        print_images(images.cpu())\n",
    "        noised_image = noise_func(images.cpu())\n",
    "        print_images(noised_image)\n",
    "        noises = torch.normal(mean, std_dev + (torch.rand(1)[0] / 200000), size=images.shape[-2:]).to(device)\n",
    "        outputs = vit_model(noised_image.to(device) + noises)\n",
    "        print_images(outputs.cpu())\n",
    "        for i in range(10):\n",
    "            outputs = vit_model(outputs + noises)\n",
    "            if i % 2 == 1:\n",
    "                print_images(outputs.cpu())\n",
    "    \n",
    "def count_noise(images, noise_func, epsilon =1e-4):\n",
    "    with torch.no_grad():\n",
    "        noised_image = noise_func(images.cpu())\n",
    "        noises = torch.normal(mean, std_dev, size=images.shape[-2:]).to(device)\n",
    "        outputs = vit_model(noised_image.to(device) + noises)\n",
    "        for i in range(100):\n",
    "            outputs = vit_model(outputs + noises)\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "        loss = criterion(outputs.cpu(), images.cpu())  # Shape: (b, c, w, h)\n",
    "        \n",
    "        loss_per_batch = loss.mean(dim=(1, 2, 3))  # Shape: (b,)\n",
    "        \n",
    "        count = torch.sum(loss_per_batch < epsilon).item()\n",
    "        \n",
    "        return(f'{count}/{images.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_noise(images, noise_func):\n",
    "    with torch.no_grad():\n",
    "        print_images(images.cpu())\n",
    "        noised_image = noise_func(images.cpu())\n",
    "        print_images(noised_image)\n",
    "        noises = torch.normal(mean, std_dev + (torch.rand(1)[0] / 200000), size=images.shape[-2:]).to(device)\n",
    "        outputs = vit_model(noised_image.to(device) + noises)\n",
    "        print_images(outputs.cpu())\n",
    "        for i in range(10):\n",
    "            outputs = vit_model(outputs + noises)\n",
    "            if i % 2 == 1:\n",
    "                print_images(outputs.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_noise_signle(images, noise_func):\n",
    "    with torch.no_grad():\n",
    "        images_lst = []\n",
    "        # images_lst.append(images.cpu())\n",
    "        noised_image = noise_func(images.cpu())\n",
    "        images_lst.append(noised_image)\n",
    "        # noises = torch.normal(mean, std_dev , size=images.shape[-2:]).to(device)\n",
    "        noises = torch.normal(mean, std_dev + (torch.rand(1)[0] / 200000), size=images.shape[-2:]).to(device)\n",
    "        outputs = vit_model(noised_image.to(device) + noises)\n",
    "        # images_lst.append(outputs.cpu())\n",
    "        for i in range(100):\n",
    "            outputs = vit_model(outputs + noises)\n",
    "            # if i % 2 == 1:\n",
    "            #     images_lst.append(outputs.cpu())\n",
    "        images_lst.append(outputs.cpu())\n",
    "        print_images(torch.stack(images_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 40\n",
    "s = sample_images#[8+index:29+index]\n",
    "# print_images(s)\n",
    "noises = []\n",
    "scores = [] \n",
    "noises.append(cross_noise(s, return_noise=True)[1])\n",
    "scores.append(count_noise(s, cross_noise))\n",
    "\n",
    "noises.append(lines_herizontal_noise(s, number_of_lines=5, noise_value=1, return_noise=True)[1])\n",
    "scores.append(count_noise(s, lambda x: lines_herizontal_noise(x, number_of_lines=5, noise_value=1)))\n",
    "\n",
    "# Square noise\n",
    "noises.append(square_noise(s, return_noise=True)[1])\n",
    "scores.append(count_noise(s, lambda x: square_noise(x)))\n",
    "\n",
    "# Triangle noise\n",
    "noises.append(tringle_noise(s, return_noise=True)[1])\n",
    "scores.append(count_noise(s, tringle_noise))\n",
    "\n",
    "# Right-side noise (default value)\n",
    "noises.append(right_side_noise(s, return_noise=True)[1])\n",
    "scores.append(count_noise(s, lambda x: right_side_noise(x)))\n",
    "\n",
    "# Right-side noise with value 1\n",
    "noises.append(right_side_noise(s, 1, return_noise=True)[1])\n",
    "scores.append(count_noise(s, lambda x: right_side_noise(x, 1)))\n",
    "\n",
    "# Bottom noise (default value)\n",
    "noises.append(bottom_noise(s, return_noise=True)[1])\n",
    "scores.append(count_noise(s, lambda x: bottom_noise(x)))\n",
    "\n",
    "# Bottom noise with value 1\n",
    "noises.append(bottom_noise(s, 1, return_noise=True)[1])\n",
    "scores.append(count_noise(s, lambda x: bottom_noise(x, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images_with_titles(images, titles):\n",
    "    # Number of images\n",
    "    n = len(images)\n",
    "    \n",
    "    # Create a figure and set the size\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(7, 4))\n",
    "    axes = axes.flatten()  # Flatten the 2D array to 1D for easy iteration\n",
    "    \n",
    "    for i in range(n):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(images[i], cmap='gray')  # Display image\n",
    "        ax.set_title(titles[i])  # Set title\n",
    "        ax.axis('off')  # Hide axis\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images_with_titles(noises, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_noise(s, lambda x: lines_herizontal_noise(x, number_of_lines=5, noise_value=1))\n",
    "print('---------------------')\n",
    "apply_noise(s, cross_noise)\n",
    "print('---------------------')\n",
    "apply_noise(s, tringle_noise)\n",
    "print('---------------------')\n",
    "# apply_noise(s, lambda x: right_side_noise(x))\n",
    "print('---------------------')\n",
    "apply_noise(s, lambda x: right_side_noise(x, 1))\n",
    "print('---------------------')\n",
    "# apply_noise(s, lambda x: bottom_noise(x))\n",
    "print('---------------------')\n",
    "apply_noise(s, lambda x: bottom_noise(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
